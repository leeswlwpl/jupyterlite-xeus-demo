{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","source":["# Pandas(iii)"],"metadata":{"id":"JgUIwqkKeySb"}},{"cell_type":"markdown","metadata":{"id":"r-mHkC50yVTC"},"source":["In this notebook we will continue using our previous sales dataset `sales-dataset-2.csv`"]},{"cell_type":"code","metadata":{"id":"YLr2_p9_yVTC"},"source":["# Load pandas and the df here\n","import pandas as pd\n","store_df = pd.read_csv(\"https://drive.google.com/u/1/uc?id=1kbCDXONvf8Bn4kEW1ssmxdJLDHC-oV-E&export=download\")\n","store_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pq7faLnayVTC"},"source":["## 1. Sorting"]},{"cell_type":"markdown","source":["### 1.1 .sort_values()"],"metadata":{"id":"fbr1BcR2ClLl"}},{"cell_type":"markdown","metadata":{"id":"cQAEy5b0yVTC"},"source":["We can reorder the rows (in a df) or values(in a series)"]},{"cell_type":"markdown","metadata":{"id":"WgrtnUtSyVTC"},"source":["```python\n","df.sort_values('column_name')\n","df.sort_values(['column_name1', 'column_name2'])\n","\n","sr.sort_values()\n","```"]},{"cell_type":"code","source":["# Demonstration\n","store_df.sort_values('Date').head()"],"metadata":{"id":"Q0bpU5gSBXNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGuB4qrjyVTC"},"source":["**Exercise**\n","\n","By checking the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html), perform the following tasks:\n","\n","1. Sort by ascending order of `Store` and `Dept` and descending order of `Date`\n","2. What is the data type of `Date`? You can check by inspecting the first element in the `Date` column. Hence, explain why the sorting in question 1 does not work.\n","3. Create a new column `Date_dt`, with the values are datetime instead of string.  Then run the sorting again. (Hint: You will need to use `.strptime()` from datetime package and `.apply()` of pandas series)"]},{"cell_type":"code","metadata":{"id":"b774TF6UyVTC"},"source":["# Type your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 .sort_index()"],"metadata":{"id":"DBZdt3V2CqBw"}},{"cell_type":"markdown","source":["It is also possible to sort by the index."],"metadata":{"id":"0U8zGqeSCv0t"}},{"cell_type":"code","source":["store_df.sort_index(ascending=False)"],"metadata":{"id":"m8PGqXT7hYJa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLgupwv4yVTC"},"source":["## 2. Aggregate functions"]},{"cell_type":"markdown","metadata":{"id":"HvNzPsajyVTD"},"source":["There are a lot of aggregation functions.  Here's some of the common ones:\n","```python\n","    series.count()\n","    series.sum()\n","    series.mean()\n","    series.max()\n","    series.min()\n","    series.unique()\n","    series.nunique()\n","```\n","Try discovering the rest of them."]},{"cell_type":"markdown","source":["**Example 1**"],"metadata":{"id":"32AvBrVEjsLI"}},{"cell_type":"code","source":["# Demonstration\n","df1 = pd.DataFrame({\n","    'Courses':[\"Spark\", \"PySpark\", \"Hadoop\", \"Python\", \"PySpark\", \"Spark\"],\n","    'Fee' :[20000, 25000, 26000, 22000, 24000, 35000],\n","    'Duration':[30, 40, 35, 40, 60, 60],\n","    'Discount':[1000, 2300, 1200, 2500, 2000, 2000]\n","})\n","df1"],"metadata":{"id":"sg53V8r9C7Wj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of unique courses\n","df1['Courses'].nunique()"],"metadata":{"id":"U9M_4_Z8irhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Average course duration\n","df1['Duration'].mean()"],"metadata":{"id":"GNU4tzSCjFFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example 2**"],"metadata":{"id":"r1UYJZlchSDX"}},{"cell_type":"markdown","source":["This is the dataframe we encountered in the first pandas lesson.\n","\n","In this example we calculate the row-sum and column-sum of the dataframe."],"metadata":{"id":"fNVQsjqQju5Y"}},{"cell_type":"code","source":["import pandas as pd\n","clinic_df = pd.DataFrame([\n","        ['January', 100, 100, 23, 100],\n","        ['February', 51, 45, 145, 45],\n","        ['March', 81, 96, 65, 96],\n","        ['April', 80, 80, 54, 180],\n","        ['May', 51, 54, 54, 154],\n","        ['June', 112, 109, 79, 129]\n","    ],\n","    columns=['month', 'clinic_east','clinic_north', 'clinic_south','clinic_west']\n",")\n","\n","with_row_sum = clinic_df.assign(row_sum=clinic_df.sum(axis=1, numeric_only=True))\n","\n","with_row_col_sum = pd.concat([with_row_sum, with_row_sum.sum(axis=0).to_frame().T])\n","\n","with_row_col_sum"],"metadata":{"id":"SN0eauEQhVY-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGPxBLhryVTC"},"source":["**Exercise**\n","\n","> Try to create some summary of the dataset `store_df`:\n","1. What is the average weekly sales?\n","2. How many difference stores are there?\n","3. Which weeks are holidays?"]},{"cell_type":"code","metadata":{"id":"4_mTDFSKtcOJ"},"source":["# Type your code below\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6C_tSkPyVTD"},"source":["## 3. Group by"]},{"cell_type":"markdown","source":["### 3.1 Groupby - aggregate pattern"],"metadata":{"id":"cs5MY8jr0dXY"}},{"cell_type":"markdown","metadata":{"id":"wusdWy4TyVTD"},"source":["Group by is an essential feature in every table manipulation. In pandas, we use the function\n","```python\n","# Group by a particular column\n","df.groupby(\"Column name\")\n","\n","```\n","After a group by, we must provide further aggregation instruction so that we can obtain usable information.\n","```python\n","# Follow the groupby by a sum()\n","df.groupby(\"Column name\").sum()\n","\n","# Follow the groupby by a max()\n","df.groupby(\"Column name\").max()\n","\n","# Follow the groupby by your own function\n","df.groupby(\"Column name\").agg(func)\n","\n","# Follow the groupby by your multiple functions\n","df.groupby(\"Column name\").agg([func1, func2])\n","\n","# Follow the groupby by different functions for different columns\n","df.groupby(\"Column name\").agg({\"col1\": func1, \"col2\": func2})\n","```"]},{"cell_type":"code","source":["# Demonstaration\n","df2 = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n","                              'Parrot', 'Parrot'],\n","                   'Max Speed': [380., 370., 24., 26.]})\n","df2"],"metadata":{"id":"SH6qInBHKPI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by, followed by aggregation\n","df2.groupby('Animal').mean()"],"metadata":{"id":"MJZEzBiUKQwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by, followed by aggregation\n","df2.groupby('Animal').agg(['sum','mean'])"],"metadata":{"id":"DYdq1uDTLN2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYIHI-_MyVTD"},"source":["**Exercise**\n","\n","1. get the sum of sales for every store\n","2. find the best performing store"]},{"cell_type":"code","metadata":{"id":"L8tJSu8DyVTD"},"source":["# Type your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Be3gDbUEyVTD"},"source":["### 3.2 The GroupBy object"]},{"cell_type":"markdown","metadata":{"id":"_ApyQMIDyVTD"},"source":["After a `.groupby()` function is run, a `DataFrameGroupBy` object is produced.\n","\n","A `DataFrameGroupBy` object represents a intermediate result where records are grouped but not aggregated yet.\n","\n","In this stage, we may further select the columns before proceed to aggregation.\n","\n","- `[['col1', 'col2]]` returns a `DataFrameGroupBy`\n","\n","- `['col']` syntax returns a `SeriesGroupBy` object"]},{"cell_type":"markdown","source":["In the following example, you can expect the result to be a `DataFrameGroupBy` object"],"metadata":{"id":"VvoLAMhvyd2T"}},{"cell_type":"code","source":["# Demonstration\n","store_df.groupby(['Store','Dept'])[['Weekly_Sales', 'Date']]"],"metadata":{"id":"Xmsd9br5ORLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 The special `count()` aggregation"],"metadata":{"id":"IVsONx5j1Crn"}},{"cell_type":"markdown","source":["Count is different from other aggregation method like `sum` and `max` because we do not need numbers to be calculated."],"metadata":{"id":"FoRFL7cz1Tq5"}},{"cell_type":"code","source":["# Demonstration\n","\n","df1 = pd.DataFrame({\n","    'Courses':[\"Spark\", \"PySpark\", \"Hadoop\", \"Python\", \"PySpark\", \"Spark\"],\n","    'Fee' :[20000, 25000, 26000, 22000, 24000, 35000],\n","    'Duration':[30, 40, 35, 40, 60, 60],\n","    'Discount':[1000, 2300, 1200, 2500, 2000, 2000]\n","})\n","df1"],"metadata":{"id":"cx_Y4ZIS1nbs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the above dataframe, to check the number of each course:\n","- Spark: 2\n","- PySpark: 2\n","- Hadoop: 1\n","- Python: 1\n","\n","The standard group by syntax will be:"],"metadata":{"id":"u_FOe1y12c89"}},{"cell_type":"code","source":["df1.groupby('Courses').count()"],"metadata":{"id":"cFdKCkF12cgA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, the result is weird since the `count()` aggregation method is applied on all the three columns `Fee`, `Duration` and `Discount`.\n","\n","Turns out the correct way is to use `.value_counts`."],"metadata":{"id":"no2zKGLL3BKp"}},{"cell_type":"code","source":["# Demonstration\n","\n","df1['Courses'].value_counts()"],"metadata":{"id":"S8GgEBt63qQd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.4 Time series Group By"],"metadata":{"id":"Xu3r_aGa5jm2"}},{"cell_type":"markdown","source":["What is time series?\n","\n","Time series is a series that represent measures in regular time interval. Examples include:\n","\n","- Per minute sleep quality\n","- Hourly temperature\n","- Daily sales volume\n"],"metadata":{"id":"qxRVIbsV62CK"}},{"cell_type":"markdown","source":["#### 3.4.1 .resample()\n","\n","While modeling time series can be very valuable, we must first preprocess them such that the interval between records are fixed.\n","\n","`.resample()` is a way to group by a time interval (e.g. 1 day)"],"metadata":{"id":"1EeXB_uH9D3W"}},{"cell_type":"markdown","source":["**Example**\n","\n","In the following dataframe, temperatures are recorded but they have irregular time intervals."],"metadata":{"id":"B-C8wUn88V7N"}},{"cell_type":"code","source":["temperature_record = pd.DataFrame([\n","    [\"1988-04-07 00:00\", 30],\n","    [\"1988-04-07 12:00\", 29],\n","    [\"1988-04-08 00:00\", 30],\n","    [\"1988-04-09 00:00\", 29],\n","    [\"1988-04-09 12:00\", 28],\n","    [\"1988-04-10 03:00\", 31],\n","], columns=['time', 'reading'])\n","\n","# to_datetime() is a short cut of .apply() + .strptime()\n","temperature_record.index = pd.to_datetime(temperature_record['time'], format=\"%Y-%m-%d %H:%M\")\n","\n","# Now the index is a DateTimeIndex - A special one for datetime\n","\n","# Then use .sample() as a group-by\n","resampled = temperature_record.resample(\"1d\")['reading'].mean()\n","\n","resampled"],"metadata":{"id":"bMSh9TA28VLR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.4.2 .rolling()\n","\n","The analytic methods \"moving average\" or \"rolling sum\" are achieved with the `.rolling()` method."],"metadata":{"id":"4xxpybAaEzqb"}},{"cell_type":"markdown","source":["**Example**\n","\n","We wish to generate a 2-days moving average on the previous dataframe.\n","\n","Note: this method does not require the index to be a datetime index."],"metadata":{"id":"uUBlq5eeHG6_"}},{"cell_type":"code","source":["resampled.rolling(2).mean()"],"metadata":{"id":"3pgwvJj_-XRI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Checkpoint**\n","\n","Why do we apply rolling to `resample` but not to `temperature_record`?\n"],"metadata":{"id":"mHQibSGQHnFR"}}]}